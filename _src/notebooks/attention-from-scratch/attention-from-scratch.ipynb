{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate_model (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using CSV, DataFrames\n",
    "using Plots\n",
    "using Embeddings\n",
    "using Random\n",
    "\n",
    "tb = CSV.read(\"small.csv\",DataFrame)\n",
    "\n",
    "const embtable = load_embeddings(GloVe{:en},1,max_vocab_size=10000)\n",
    "const get_word_index = Dict(word=>ii for (ii,word) in enumerate(embtable.vocab))\n",
    "\n",
    "# Returns embeddings for word\n",
    "function get_embeddings(word)\n",
    "    return embtable.embeddings[:,get_word_index[word]]\n",
    "end\n",
    "\n",
    "# Splits sentence into a vector of words\n",
    "function word_tokeniser(sentence)\n",
    "    return split(sentence,\" \")\n",
    "end\n",
    "\n",
    "# Softmax function\n",
    "function softmax(x)\n",
    "    x = x .- maximum(x)\n",
    "    return exp.(x) ./ sum(exp.(x))\n",
    "end\n",
    "\n",
    "# Cross Entropy Loss\n",
    "function CrossEntropyLoss(z,x)\n",
    "    return -sum(x.*log.(z))\n",
    "end\n",
    "\n",
    "# Linear Transformation\n",
    "function LinearTransform(x,W,b)\n",
    "    return W*x.+b\n",
    "end\n",
    "\n",
    "# Feedforward network\n",
    "function FeedForward(x,W,b)\n",
    "    return LinearTransform(x,W,b) \n",
    "end\n",
    "\n",
    "# Return Attention Weights\n",
    "function AttentionWeights(x,q,k,v)\n",
    "    # compute similarity between queries and keys (with scaling)\n",
    "    e = q'*k/sqrt(length(q))\n",
    "\n",
    "    # initialize attention weight matrix α with zeroes\n",
    "    α = zeros(size(e))\n",
    "\n",
    "    # normalize each similarity row with softmax\n",
    "    for row in 1:size(e)[1]\n",
    "        α[row,:] = softmax(e[row,:])\n",
    "    end    \n",
    "    return α\n",
    "end\n",
    "\n",
    "# Attention block\n",
    "function Attention(x,Q,Qb,K,Kb,V,Vb)\n",
    "    # queries\n",
    "    q = LinearTransform(x,Q,Qb)\n",
    "    # keys\n",
    "    k = LinearTransform(x,K,Kb) \n",
    "    # values\n",
    "    v = LinearTransform(x,V,Vb)\n",
    "\n",
    "    # Attention Weights\n",
    "    α = AttentionWeights(x,q,k,v)\n",
    "\n",
    "    # context vectors\n",
    "    z = v * α' \n",
    "    return q,k,v,α,z\n",
    "end\n",
    "\n",
    "# Forward propagate\n",
    "function forwardprop(x,Q,Qb,K,Kb,V,Vb,W,b)\n",
    "    # Reshape from 1d to 2d\n",
    "    if ndims(x)==1\n",
    "        x = reshape(x,(:,1))  \n",
    "    end\n",
    "    x = vcat(x,Vector(range(0,size(x)[2]-1))') \n",
    "    \n",
    "    # Return attention values\n",
    "    q,k,v,α,z = Attention(x,Q,Qb,K,Kb,V,Vb)\n",
    "\n",
    "    # Feed Forward layer\n",
    "    f = FeedForward(z,W,b)  #shape: [3xn]\n",
    "\n",
    "    # Average pooling. \n",
    "    p = sum(f,dims=2)/size(f)[2]  #shape: [3x1]\n",
    "\n",
    "    # Softmax layer to get probabilities \n",
    "    p = softmax(p)\n",
    "\n",
    "    return x,q,k,v,α,z,p\n",
    "end\n",
    "\n",
    "# Train step\n",
    "function train(x,y,train_params...)\n",
    "    x,q,k,v,α,z,p = forwardprop(x,train_params...) \n",
    "    CEloss = CrossEntropyLoss(p,y)\n",
    "    train_params = backprop(x,y,train_params...,q,k,v,α,z,p)\n",
    "    return train_params...,CEloss\n",
    "end\n",
    "\n",
    "# Backpropagate\n",
    "function backprop(x,y,\n",
    "                Q,Qb,K,Kb,V,Vb,W,b,\n",
    "                q,k,v,α,z,p,\n",
    "                η=.001)\n",
    "    # Softmax gradient ∂L/∂σ\n",
    "    ∂L_∂p = p-y    #shape: [3x1] \n",
    "\n",
    "    # Average pooling gradient ∂L/∂f\n",
    "    ∂p_∂f = (1 ./size(z)[2] .*ones(1,size(z)[2]))\n",
    "    ∂L_∂f = ∂L_∂p*∂p_∂f  #shape: [3xn] \n",
    "    \n",
    "    # NN local gradients ∂f/∂z, ∂f/∂W\n",
    "    ∂f_∂z = W  #shape: [3xd] \n",
    "    ∂f_∂W = z' #shape: [4xd]\n",
    "\n",
    "    # NN gradients ∂L/∂W and ∂L/∂b\n",
    "    ∂L_∂W = ∂L_∂f*∂f_∂W  #shape: [3xd]  \n",
    "    ∂L_∂b = sum(∂L_∂f,dims=2)  #shape: [3x1] \n",
    "\n",
    "    # Context vector gradients\n",
    "    ∂L_∂z = (∂L_∂f'*∂f_∂z)' #shape: [dxn]  \n",
    "\n",
    "    # Attention gradients\n",
    "    # Local value gradients ∂z/∂v, ∂v/∂V  \n",
    "    ∂z_∂v = α  #shape: [nxn] \n",
    "    ∂v_∂V = x' #shape: [nxd]\n",
    "\n",
    "    # Local attention weight gradients ∂z/∂α \n",
    "    ∂z_∂α = v  #shape: [dxn] \n",
    "\n",
    "    # Initialize ∂α/∂e to zeroes\n",
    "    ∂α_∂e = zeros(size(α)[1],size(α)[2])  #shape: [nxn]\n",
    "\n",
    "    # Derivative of softmax\n",
    "    for k in 1:size(α)[1]\n",
    "        for j in 1:size(α)[2]\n",
    "            if j == k\n",
    "                ∂α_∂e[j,k] = α[j]*(1-α[j]) \n",
    "            else\n",
    "                ∂α_∂e[j,k] = -α[k]*α[j]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Local query, key gradients ∂e_∂q, ∂e_∂k \n",
    "    ∂e_∂q, ∂e_∂k = k', q'  #shape: [nxd],[nxd] \n",
    "    ∂q_∂Q, ∂k_∂K = x', x'  #shape: [nxd],[nxd]  \n",
    "\n",
    "    # Softmax gradients\n",
    "    ∂L_∂α = ∂L_∂z'*∂z_∂α   #shape: [nxn]\n",
    "\n",
    "    # Similarity score gradients\n",
    "    ∂L_∂e = ∂L_∂α*∂α_∂e    #shape: [nxn] \n",
    "\n",
    "    # query gradients\n",
    "    ∂L_∂q = ∂L_∂e*∂e_∂q  #shape: [nxd]\n",
    "    # key gradients\n",
    "    ∂L_∂k = ∂L_∂e'*∂e_∂k #shape: [nxd] \n",
    "    # values gradients\n",
    "    ∂L_∂v = ∂L_∂z*∂z_∂v  #shape: [dxn]\n",
    "\n",
    "    # Q,K,V parameter gradients \n",
    "    ∂L_∂Q = ∂L_∂q'*∂q_∂Q  #shape: [dxd]\n",
    "    ∂L_∂K = ∂L_∂k'*∂k_∂K  #shape: [dxd]\n",
    "    ∂L_∂V = ∂L_∂v*∂v_∂V   #shape: [dxd]\n",
    "\n",
    "    ∂L_∂Qb = sum(∂L_∂q',dims=2)  #shape: [dx1]\n",
    "    ∂L_∂Kb = sum(∂L_∂k',dims=2) #shape: [dx1]\n",
    "    ∂L_∂Vb = sum(∂L_∂v,dims=2)  #shape: [dx1]\n",
    "\n",
    "    # Update Attention parameters\n",
    "    # Initialize new parameter matrices with current parameters\n",
    "    Q_new = Q\n",
    "    Qb_new = Qb\n",
    "    K_new = K \n",
    "    Kb_new = Kb\n",
    "    V_new = V\n",
    "    Vb_new = Vb\n",
    "    W_new = W\n",
    "    b_new = b\n",
    "\n",
    "    # Update all trainable parameters with SGD\n",
    "    Q_new = Q_new .- η * ∂L_∂Q\n",
    "    Qb_new = Qb_new .- η * ∂L_∂Qb\n",
    "    \n",
    "    K_new = K_new .- η * ∂L_∂K   \n",
    "    Kb_new = Kb_new .- η * ∂L_∂Kb\n",
    "    \n",
    "    V_new = V_new .- η * ∂L_∂V \n",
    "    Vb_new = Vb_new .- η * ∂L_∂Vb\n",
    "\n",
    "    W_new = W_new #.- η * ∂L_∂W\n",
    "    b_new = b_new #.- η * ∂L_∂b\n",
    "\n",
    "    return Q_new,Qb_new,K_new,Kb_new,V_new,Vb_new,W_new,b_new\n",
    "end\n",
    "\n",
    "# Removes words that are not in dictionary\n",
    "function remove_nid(sentence)\n",
    "    sen = []\n",
    "    if !ismissing(sentence)\n",
    "        for i in word_tokeniser(sentence)\n",
    "            try get_embeddings(i)\n",
    "                push!(sen,i)\n",
    "            catch e\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return sen\n",
    "end\n",
    "\n",
    "# Evaluates the sentiment given a sentence as input\n",
    "function evaluate_model(sen)\n",
    "    x_em = []\n",
    "    sen = remove_nid(sen)\n",
    "    for i in (sen)\n",
    "        if length(x_em) == 0\n",
    "            x_em = get_embeddings(i)\n",
    "        else \n",
    "            x_em = hcat(x_em,get_embeddings(i))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    α = forwardprop(x_em,Q,Qb,K,Kb,V,Vb,W,b)[5]\n",
    "    #println(α)\n",
    "    #println(forwardprop(x_em,Q,Qb,K,Kb,V,Vb,W,b)[end])\n",
    "    # plot heatmap of α\n",
    "    heatmap(sen,sen,α,clims=(0,1),aspect_ratio=1,color=:deepsea,\n",
    "            title=\"Attention weights α\",grid=\"off\")\n",
    "    \n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main \n",
    "# Random seed for reproducibility\n",
    "rng = MersenneTwister(12);\n",
    "\n",
    "# Initialize small random parameter values\n",
    "Q = randn(rng, (51, 51))/100\n",
    "Qb = zeros(51,1)\n",
    "K = randn(rng, (51, 51))/100\n",
    "Kb = zeros(51,1)\n",
    "V = K\n",
    "Vb = zeros(51,1)\n",
    "W = randn(rng, (3, 51))/100\n",
    "b = zeros(3,1) \n",
    "\n",
    "# Sentiment dictionary that converts sentiment\n",
    "# text into one-hot labels\n",
    "sent_dict = Dict(\"positive\"=>[0,0,1],\"negative\"=>[1,0,0],\"neutral\"=>[0,1,0])\n",
    "\n",
    "#training\n",
    "for epoch=1:1000\n",
    "    total_l = 0   #total loss\n",
    "    for idx in 1:nrow(tb)\n",
    "        x_em = []\n",
    "        l = 0   #current loss\n",
    "        sen = tb[idx,\"cleaned_review\"]  #gets sentence\n",
    "        sen = remove_nid(sen)  #remove words not in dictionary\n",
    "        if length(sen)!=0\n",
    "            for i in (sen)\n",
    "                if length(x_em) == 0\n",
    "                    x_em = get_embeddings(i)\n",
    "                else \n",
    "                    #Concatenate word embeddings along columns\n",
    "                    x_em = hcat(x_em,get_embeddings(i)) \n",
    "                end\n",
    "            end\n",
    "            #One hot vector sentiment\n",
    "            y = sent_dict[tb[idx,\"sentiments\"]]\n",
    "            #Update parameters\n",
    "            Q,Qb,K,Kb,V,Vb,W,b,l = train(x_em,y,Q,Qb,K,Kb,V,Vb,W,b)\n",
    "        end\n",
    "        total_l += l\n",
    "    end\n",
    "    println(\"Total loss:\", total_l/nrow(tb))\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vizualize attention weights\n",
    "evaluate_model(\"very sad as they both fail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"he loved that plug with good price \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"terrible quality for this price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"i love this fantastic product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"easy to move around\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
